# ============================================
#    Authorship Detection - Final Project 2025
# ============================================

# ------------ SETUP ------------
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk import ngrams
from collections import Counter

# Download NLTK data
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")

# ============================================
#          1) DATA SELECTION
# ============================================

texts = {
    "Shakespeare": [
        "To be, or not to be: that is the question.",
        "All the world's a stage, and all the men and women merely players.",
        "Some are born great, some achieve greatness, and some have greatness thrust upon them.",
        "The course of true love never did run smooth.",
        "Love all, trust a few, do wrong to none.",
        "Cowards die many times before their deaths; the valiant never taste of death but once."
    ],

    "Jane Austen": [
        "It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.",
        "A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony.",
        "There is no charm equal to tenderness of heart.",
        "I declare after all there is no enjoyment like reading!",
        "Vanity and pride are different things, though the words are often used synonymously.",
        "One half of the world cannot understand the pleasures of the other."
    ],

    "Mark Twain": [
        "The secret of getting ahead is getting started.",
        "Kindness is the language which the deaf can hear and the blind can see.",
        "Truth is stranger than fiction, but it is because Fiction is obliged to stick to possibilities.",
        "The lack of money is the root of all evil.",
        "The best way to cheer yourself up is to try to cheer somebody else up.",
        "Courage is resistance to fear, mastery of fear, not absence of fear."
    ],

    "Charles Dickens": [
        "It was the best of times, it was the worst of times.",
        "Have a heart that never hardens, and a temper that never tires.",
        "A loving heart is the truest wisdom.",
        "No one is useless in this world who lightens the burden of another.",
        "There is a wisdom of the head, and a wisdom of the heart.",
        "We forge the chains we wear in life."
    ],

    "Edgar Allan Poe": [
        "I became insane, with long intervals of horrible sanity.",
        "All that we see or seem is but a dream within a dream.",
        "Words have no power to impress the mind without the exquisite horror of their reality.",
        "Those who dream by day are cognizant of many things which escape those who dream only by night.",
        "The boundaries which divide Life from Death are at best shadowy and vague.",
        "There is no beauty without some strangeness."
    ]
}

# ============================================
#          2) PREPROCESSING FUNCTION
# ============================================

stop_words = set(stopwords.words("english"))
punct = string.punctuation
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    sentences = sent_tokenize(text)
    cleaned_sentences = []

    for sent in sentences:
        tokens = word_tokenize(sent.lower())
        cleaned = []

        for w in tokens:
            if w in punct or w.isdigit():
                continue
            if w in stop_words:
                continue
            w = lemmatizer.lemmatize(w)
            cleaned.append(w)

        cleaned_sentences.append(cleaned)

    return cleaned_sentences

# Apply preprocessing
processed = {}
print("\n=== Preprocessing Output ===\n")
for author in texts:
    processed[author] = []
    print(f"Author: {author}")
    for sentence in texts[author]:
        result = preprocess(sentence)
        processed[author].append(result)
        print(result)
    print("\n")

# ============================================
#          3) BIGRAM MODEL FUNCTIONS
# ============================================

def build_bigram_model(tokens):
    bigram_list = list(ngrams(tokens, 2))
    bigram_freq = Counter(bigram_list)
    unigram_freq = Counter(tokens)
    return bigram_freq, unigram_freq

def bigram_probability(tokens, bigram_freq, unigram_freq):
    prob = 1
    for w1, w2 in ngrams(tokens, 2):
        count_bigram = bigram_freq[(w1, w2)]
        count_unigram = unigram_freq[w1]
        if count_bigram == 0:
            prob *= 0.0001  # smoothing
        else:
            prob *= count_bigram / count_unigram
    return prob

# ============================================
#          4) SELECT 10 SENTENCES FOR N-GRAM
# ============================================

selected_sentences = []
for author in texts:
    for s in texts[author]:
        selected_sentences.append(s)
selected_sentences = selected_sentences[:10]

print("=== Selected 10 Sentences ===")
for i, s in enumerate(selected_sentences):
    print(f"{i+1}. {s}")

# ============================================
#          5) CALCULATE BIGRAM PROBABILITY
# ============================================

print("\n\n=== Sentence Probability (Bigram Model) ===\n")
for sent in selected_sentences:
    tokens = word_tokenize(sent.lower())
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w not in punct]

    bigram_freq, unigram_freq = build_bigram_model(tokens)
    prob = bigram_probability(tokens, bigram_freq, unigram_freq)

    print(f"Sentence: {sent}")
    print("Tokens:", tokens)
    print(f"Probability = {prob}\n")

# ============================================
#          END OF FINAL PROJECT
# ============================================
