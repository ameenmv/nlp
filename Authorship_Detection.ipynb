# Authorship Detection - Final Project 2025
# Author: Ameen Mohamed
# Project: Data Preprocessing & N-Gram Probability


# setup
import random, re, math
from collections import Counter
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize

random.seed(42)

#=====================================
#1) DATA SELECTION (AUTHORS DATASET) =>


texts = {
    "Shakespeare": [
        "To be, or not to be: that is the question.",
        "All the world's a stage, and all the men and women merely players.",
        "Some are born great, some achieve greatness, and some have greatness thrust upon them.",
        "The course of true love never did run smooth.",
        "Love all, trust a few, do wrong to none.",
        "Cowards die many times before their deaths; the valiant never taste of death but once."
    ],

    "Jane Austen": [
        "It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.",
        "A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony.",
        "There is no charm equal to tenderness of heart.",
        "I declare after all there is no enjoyment like reading!",
        "Vanity and pride are different things, though the words are often used synonymously.",
        "One half of the world cannot understand the pleasures of the other."
    ],

    "Mark Twain": [
        "The secret of getting ahead is getting started.",
        "Kindness is the language which the deaf can hear and the blind can see.",
        "Truth is stranger than fiction, but it is because Fiction is obliged to stick to possibilities.",
        "The lack of money is the root of all evil.",
        "The best way to cheer yourself up is to try to cheer somebody else up.",
        "Courage is resistance to fear, mastery of fear, not absence of fear."
    ],

    "Charles Dickens": [
        "It was the best of times, it was the worst of times.",
        "Have a heart that never hardens, and a temper that never tires.",
        "A loving heart is the truest wisdom.",
        "No one is useless in this world who lightens the burden of another.",
        "There is a wisdom of the head, and a wisdom of the heart.",
        "We forge the chains we wear in life."
    ],

    "Edgar Allan Poe": [
        "I became insane, with long intervals of horrible sanity.",
        "All that we see or seem is but a dream within a dream.",
        "Words have no power to impress the mind without the exquisite horror of their reality.",
        "Those who dream by day are cognizant of many things which escape those who dream only by night.",
        "The boundaries which divide Life from Death are at best shadowy and vague.",
        "There is no beauty without some strangeness."
    ]
}

#===========================
#2) PREPROCESSING FUNCTION =>

# NLTK Lemmatizer
STOPWORDS = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

all_texts = []
for author in texts:
    all_texts.extend(texts[author])

def normalize_and_lemmatize(text):
    # 1. Tokenization and Lowercase
    tokens = word_tokenize(text.lower())
    cleaned = []

    # 2. Remove Punctuation and Numbers
    # 3. Remove Stopwords
    for w in tokens:
        if w.isalpha(): # remove punctuatuion and numbers
            if w not in STOPWORDS:
                # 4. Lemmatization
                cleaned.append(lemmatizer.lemmatize(w))

    return cleaned

# apply processing to all text
token_lists = []
for text in all_texts:
    token_lists.append(normalize_and_lemmatize(text))

#=========================================
#3) BIGRAM MODEL & PROBABILITY FUNCTIONS =>

START, END = "<s>", "</s>"

def build_bigram_model(token_lists):

    unigram, bigram = Counter(), Counter()
    for toks in token_lists:
        # add start and end
        seq = [START] + toks + [END]
        unigram.update(seq)
        # Bigrams
        bigram.update(zip(seq[:-1], seq[1:]))

    vocab = set(unigram.keys())
    V = len(vocab)

    def prob(w_prev, w_cur):
        # Laplace Smoothing: P(w_cur | w_prev) = (Count(w_prev, w_cur) + 1) / (Count(w_prev) + V)
        return (bigram[(w_prev, w_cur)] + 1) / (unigram[w_prev] + V)

    return dict(unigram=unigram, bigram=bigram, vocab=vocab, V=V, prob=prob)


def sentence_bigram_logprob(tokens, model):
    # المعادلة

    # إضافة START و END
    seq = [START] + tokens + [END]
    logp = 0.0

    # حساب Prob لكل Bigram
    for w_prev, w_cur in zip(seq[:-1], seq[1:]):
        p = model["prob"](w_prev, w_cur)
        logp += math.log(p) # إضافة اللوج

    return logp

# بناء النموذج على كل الداتا
bigram_model = build_bigram_model(token_lists)

# ============================================
#4) CALCULATE PROBABILITY FOR 10 SENTENCES =>

all_sentences = []
for author, sentences in texts.items():
    all_sentences.extend(sentences)

# select first 10 sentences
selected_sentences = all_sentences[:10]

print("=== Selected 10 Sentences & Their Bigram Log Probability ===\n")
results = []

for original_text in selected_sentences:
    # reprocessing on selected sentence
    tokens = normalize_and_lemmatize(original_text)

    # compute log probability
    logp = sentence_bigram_logprob(tokens, bigram_model)

    results.append({
        "Original Text": original_text,
        "Tokens (After Preprocessing)": " ".join(tokens),
        "Bigram Log Probability": logp
    })

# display results on clean table
results_df = pd.DataFrame(results).sort_values("Bigram Log Probability", ascending=False)
print(results_df.to_string(index=False))

# ============================================
#          END OF FINAL PROJECT
# ============================================
